# # ------------------------
# # Core backend
# # ------------------------
# fastapi
# uvicorn[standard]
# python-multipart
# pydantic
# rq
# redis
# pymongo
# boto3
# pillow

# # -----------------------
# # ML / CV (PINNED)
# # ------------------------
# numpy>=1.24,<2.0
# torch==2.1.2
# torchvision==0.16.2

# opencv-python-headless<4.12
# mediapipe==0.10.9
# scikit-learn==1.4.2


# facenet-pytorch==2.5.3
# # Audio sync
# librosa==0.10.2.post1
# soundfile==0.12.1
# git+https://github.com/openai/CLIP.git





# ============================================================
# CORE BACKEND
# ============================================================



fastapi==0.110.0
uvicorn[standard]==0.27.1
python-multipart==0.0.9
pydantic==2.6.1
rq==1.15.1
redis==5.0.1
pymongo==4.6.1
boto3==1.34.34
pillow==10.2.0


# ============================================================
# NUMERICAL / BASE
# ============================================================

numpy>=1.24,<2.0
scipy==1.11.4


# ============================================================
# PYTORCH STACK (CPU)
# ============================================================

torch==2.1.2
torchvision==0.16.2


# ============================================================
# COMPUTER VISION
# ============================================================

opencv-python-headless<4.12
mediapipe==0.10.9
scikit-learn==1.4.2


# ============================================================
# FACE RECOGNITION STACK (REQUIRES CMAKE + BUILD-ESSENTIAL)
# ============================================================




facenet-pytorch==2.5.3


# ============================================================
# AUDIO PROCESSING
# ============================================================

librosa==0.10.2.post1
soundfile==0.12.1


# ============================================================
# CLIP (USED FOR VISION EMBEDDINGS)
# ============================================================


